{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968127ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T08:56:24.180419Z",
     "iopub.status.busy": "2022-09-17T08:56:24.179832Z",
     "iopub.status.idle": "2022-09-17T08:56:24.214731Z",
     "shell.execute_reply": "2022-09-17T08:56:24.212199Z",
     "shell.execute_reply.started": "2022-09-17T08:56:24.180306Z"
    },
    "papermill": {
     "duration": 0.008954,
     "end_time": "2022-09-25T13:17:16.165534",
     "exception": false,
     "start_time": "2022-09-25T13:17:16.156580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**In this notebook, i'm going to explain L2 regularization and show an example of CNN model being regularized** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147c420a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:16.183692Z",
     "iopub.status.busy": "2022-09-25T13:17:16.182838Z",
     "iopub.status.idle": "2022-09-25T13:17:21.735500Z",
     "shell.execute_reply": "2022-09-25T13:17:21.734208Z"
    },
    "papermill": {
     "duration": 5.564884,
     "end_time": "2022-09-25T13:17:21.738303",
     "exception": false,
     "start_time": "2022-09-25T13:17:16.173419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow import keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.optimizers\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    " #   for filename in filenames:\n",
    "  #      print(os.path.join(dirname, filename))\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb5758",
   "metadata": {
    "papermill": {
     "duration": 0.008621,
     "end_time": "2022-09-25T13:17:21.755779",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.747158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# L2 Regularization (for a CNN example)\n",
    "##### also called simply “weight decay\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd786f79",
   "metadata": {
    "papermill": {
     "duration": 0.007499,
     "end_time": "2022-09-25T13:17:21.771487",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.763988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**If you suspect your neural network is overfitting your data, \n",
    "that is, you have a high variance problem, one of the first things you should try is probably regularization.**  \n",
    "\n",
    "In L2, we have:\n",
    "  \n",
    "![L2](https://i.ibb.co/b3Dp6mL/L2.png) it comes **to adding a cost** to the loss function for large weights\n",
    "\n",
    "Here, **lambda** is the regularization parameter. It is the hyperparameter whose value is optimized for better results. L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).  \n",
    "**By default, no regularizer is used in any layers.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2315fe9",
   "metadata": {
    "papermill": {
     "duration": 0.008039,
     "end_time": "2022-09-25T13:17:21.788211",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.780172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A weight regularizer can be added to each layer when the layer is defined in a Keras model.  \n",
    "**A weight regularizer can be added to each layer when the layer is defined in a Keras model.**\n",
    "\n",
    "This is achieved by setting the kernel_regularizer argument on each layer. A separate regularizer can also be used for the bias via the bias_regularizer argument, although this is less often used.\n",
    "\n",
    "Let’s look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e9ad4",
   "metadata": {
    "papermill": {
     "duration": 0.007565,
     "end_time": "2022-09-25T13:17:21.803311",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.795746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Weight Regularization for Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9718e850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:21.820300Z",
     "iopub.status.busy": "2022-09-25T13:17:21.819712Z",
     "iopub.status.idle": "2022-09-25T13:17:21.824339Z",
     "shell.execute_reply": "2022-09-25T13:17:21.823285Z"
    },
    "papermill": {
     "duration": 0.015478,
     "end_time": "2022-09-25T13:17:21.826414",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.810936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## example of l2 on a dense layer\n",
    "##from keras.layers import Dense\n",
    "##from keras.regularizers import l2\n",
    "\n",
    "##model.add(Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2378f02",
   "metadata": {
    "papermill": {
     "duration": 0.007877,
     "end_time": "2022-09-25T13:17:21.842254",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.834377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "kernel_regularizer: Regularizer to apply a penalty on the layer's kernel  \n",
    "bias_regularizer: Regularizer to apply a penalty on the layer's bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239d145",
   "metadata": {
    "papermill": {
     "duration": 0.007855,
     "end_time": "2022-09-25T13:17:21.858209",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.850354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Weight Regularization for Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd7e9b",
   "metadata": {
    "papermill": {
     "duration": 0.007648,
     "end_time": "2022-09-25T13:17:21.873571",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.865923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Like the Dense layer, the Convolutional layers (e.g. Conv1D and Conv2D) also use the kernel_regularizer and bias_regularizer arguments to define a regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677d540d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:21.890848Z",
     "iopub.status.busy": "2022-09-25T13:17:21.890520Z",
     "iopub.status.idle": "2022-09-25T13:17:21.895745Z",
     "shell.execute_reply": "2022-09-25T13:17:21.894935Z"
    },
    "papermill": {
     "duration": 0.01661,
     "end_time": "2022-09-25T13:17:21.897933",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.881323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of l2 on a convolutional layer\n",
    "#from keras.layers import Conv2D\n",
    "#from keras.regularizers import l2\n",
    "\n",
    "#model.add(Conv2D(32, (3,3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65cfec",
   "metadata": {
    "papermill": {
     "duration": 0.007484,
     "end_time": "2022-09-25T13:17:21.913149",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.905665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Weight Regularization for Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be0763",
   "metadata": {
    "papermill": {
     "duration": 0.008025,
     "end_time": "2022-09-25T13:17:21.928737",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.920712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Example for Recccurrent Layers :   \n",
    "LSTM layer  \n",
    "GRU layer  \n",
    "SimpleRNN layer  \n",
    "TimeDistributed layer  \n",
    "Bidirectional layer  \n",
    "ConvLSTM1D layer  \n",
    "ConvLSTM2D layer  \n",
    "ConvLSTM3D layer  \n",
    "Base RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22eb7c",
   "metadata": {
    "papermill": {
     "duration": 0.007718,
     "end_time": "2022-09-25T13:17:21.944088",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.936370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Recurrent layers like the LSTM offer more flexibility in regularizing the weights.\n",
    "\n",
    "The input, recurrent, and bias weights can all be regularized separately via the kernel_regularizer, recurrent_regularizer, and bias_regularizer arguments.\n",
    "\n",
    "The example below sets an l2 regularizer on an LSTM recurrent layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecbf1d",
   "metadata": {
    "papermill": {
     "duration": 0.007586,
     "end_time": "2022-09-25T13:17:21.959581",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.951995",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1989f717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:21.975998Z",
     "iopub.status.busy": "2022-09-25T13:17:21.975694Z",
     "iopub.status.idle": "2022-09-25T13:17:21.980806Z",
     "shell.execute_reply": "2022-09-25T13:17:21.980007Z"
    },
    "papermill": {
     "duration": 0.015657,
     "end_time": "2022-09-25T13:17:21.982784",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.967127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of l2 on an lstm layer\n",
    "#from keras.layers import LSTM\n",
    "#from keras.regularizers import l2\n",
    "#model.add(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e728f3",
   "metadata": {
    "papermill": {
     "duration": 0.007616,
     "end_time": "2022-09-25T13:17:21.998058",
     "exception": false,
     "start_time": "2022-09-25T13:17:21.990442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Which layers should i add L2 regularization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45f272",
   "metadata": {
    "papermill": {
     "duration": 0.007551,
     "end_time": "2022-09-25T13:17:22.013235",
     "exception": false,
     "start_time": "2022-09-25T13:17:22.005684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "it seems to be an convention to add L2 regularization for each layer, as long as you didn’t set a too large weight for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92526f",
   "metadata": {
    "papermill": {
     "duration": 0.007944,
     "end_time": "2022-09-25T13:17:22.028849",
     "exception": false,
     "start_time": "2022-09-25T13:17:22.020905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To increase the regularization of a model(=the effect of the L_2 penalization) one of the actions that you can do **is increasing the value of the hyperparameter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334c348",
   "metadata": {
    "papermill": {
     "duration": 0.00763,
     "end_time": "2022-09-25T13:17:22.044386",
     "exception": false,
     "start_time": "2022-09-25T13:17:22.036756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b6182",
   "metadata": {
    "papermill": {
     "duration": 0.007514,
     "end_time": "2022-09-25T13:17:22.059538",
     "exception": false,
     "start_time": "2022-09-25T13:17:22.052024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "to be added on the dropout part ::  \n",
    "Typically there is no need to to add dropout for every layer. In most of the popular CNN structure, you may only add dropout at each (or only the last) full connected layer.\n",
    "\n",
    "Adding too much dropout for regularization will severely slow down the convergence rate, and change over-fitting to under-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d333428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:22.078179Z",
     "iopub.status.busy": "2022-09-25T13:17:22.076702Z",
     "iopub.status.idle": "2022-09-25T13:17:25.204281Z",
     "shell.execute_reply": "2022-09-25T13:17:25.203285Z"
    },
    "papermill": {
     "duration": 3.139468,
     "end_time": "2022-09-25T13:17:25.206973",
     "exception": false,
     "start_time": "2022-09-25T13:17:22.067505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 13:17:22.423552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:22.543300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:22.544143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:22.545451: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-25 13:17:22.553982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:22.554835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:22.555664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:24.685840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:24.686724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:24.687428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-25 13:17:24.688055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "model0 = ks.models.Sequential()\n",
    "model0.add(ks.layers.Dense(4, input_shape=(64, 64, 1))) #units=4(we define how many outputs we want)\n",
    "model0.add(ks.layers.Dense(8, input_shape=(64, 64, 1)))\n",
    "model0.add(ks.layers.Dense(8, input_shape=(64, 64, 1)))\n",
    "model0.add(ks.layers.Conv2D(64,(3, 3), activation='relu')) \n",
    "       \n",
    "model0.add(ks.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "model0.add(ks.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model0.add(ks.layers.MaxPooling2D(2, 2)) \n",
    "       \n",
    "model0.add(ks.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model0.add(ks.layers.MaxPooling2D(2, 2))\n",
    "model0.add(ks.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model0.add(ks.layers.MaxPooling2D(2, 2))\n",
    "model0.add(ks.layers.Dropout(0.2))\n",
    "model0.add(ks.layers.Flatten())\n",
    "\n",
    "model0.add(ks.layers.Dense(1024, activation='relu'))\n",
    "model0.add(ks.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model0.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adae49dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:25.226948Z",
     "iopub.status.busy": "2022-09-25T13:17:25.225511Z",
     "iopub.status.idle": "2022-09-25T13:17:32.769884Z",
     "shell.execute_reply": "2022-09-25T13:17:32.768919Z"
    },
    "papermill": {
     "duration": 7.556025,
     "end_time": "2022-09-25T13:17:32.772291",
     "exception": false,
     "start_time": "2022-09-25T13:17:25.216266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This is more explained on my notebook: https://www.kaggle.com/code/houssemaminetouihri/implementation-of-cnn-s-models-and-comparison-betw#Importing-necessary-libraries\n",
    "#Reading the data\n",
    "train=pd.read_csv(\"../input/sign-recognition/Train.csv\")\n",
    "test=pd.read_csv(\"../input/sign-recognition/Test.csv\")\n",
    "#Drop ID column and define train_y , train_x , test_x\n",
    "train_y=train['Target'].astype('float32')\n",
    "train_x=train.drop(['Target','ID'],axis=1).astype('int32')\n",
    "test_x=test.drop(['ID'],axis=1).astype('int32')\n",
    "#One Hot Encoding for train_y\n",
    "train_y=ks.utils.to_categorical(train_y,10)\n",
    "#Reshape the size of images and normalize them\n",
    "train_x=train_x.values.reshape(-1,64,64,1) #.values return an array #-1:to convert it to 1-D \n",
    "train_x=train_x/255.0\n",
    "test_x=test_x.values.reshape(-1,64,64,1)\n",
    "test_x/255.0\n",
    "#split\n",
    "x_train , x_test, y_train , y_test=train_test_split(train_x, train_y , test_size=0.15)\n",
    "#Generating new images\n",
    "datagenerator = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "train_generator = datagenerator.flow(x_train,y_train,)\n",
    "validation_generator = datagenerator.flow(x_test, y_test)\n",
    "model0.compile(optimizer=ks.optimizers.Adam(),loss='categorical_crossentropy',metrics=['acc'])\n",
    "class myCallback(ks.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc')>0.85) and (logs.get('val_acc')>0.85):\n",
    "            print('\\n reached 84% accuarcy so stopping training')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f0b133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:17:32.789832Z",
     "iopub.status.busy": "2022-09-25T13:17:32.789543Z",
     "iopub.status.idle": "2022-09-25T13:23:23.097027Z",
     "shell.execute_reply": "2022-09-25T13:23:23.095966Z"
    },
    "papermill": {
     "duration": 350.318971,
     "end_time": "2022-09-25T13:23:23.099452",
     "exception": false,
     "start_time": "2022-09-25T13:17:32.780481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2022-09-25 13:17:32.873511: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 13:17:35.311329: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 11s 28ms/step - loss: 1.9129 - acc: 0.2621 - val_loss: 1.2787 - val_acc: 0.4853\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.9842 - acc: 0.5892 - val_loss: 0.8229 - val_acc: 0.6680\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.7413 - acc: 0.7049 - val_loss: 0.6739 - val_acc: 0.7147\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 4s 31ms/step - loss: 0.6363 - acc: 0.7358 - val_loss: 0.6124 - val_acc: 0.7693\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.5749 - acc: 0.7687 - val_loss: 0.5856 - val_acc: 0.7600\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.5720 - acc: 0.7755 - val_loss: 0.5817 - val_acc: 0.7813\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.5367 - acc: 0.7873 - val_loss: 0.5799 - val_acc: 0.7840\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 0.5292 - acc: 0.7962 - val_loss: 0.5099 - val_acc: 0.8147\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.5046 - acc: 0.8089 - val_loss: 0.5066 - val_acc: 0.8253\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4978 - acc: 0.8165 - val_loss: 0.5227 - val_acc: 0.8213\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.4850 - acc: 0.8172 - val_loss: 0.5250 - val_acc: 0.8213\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4741 - acc: 0.8212 - val_loss: 0.4943 - val_acc: 0.8360\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4800 - acc: 0.8214 - val_loss: 0.4975 - val_acc: 0.8280\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 4s 31ms/step - loss: 0.4885 - acc: 0.8209 - val_loss: 0.5630 - val_acc: 0.8227\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.5037 - acc: 0.8104 - val_loss: 0.5204 - val_acc: 0.8213\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4666 - acc: 0.8264 - val_loss: 0.5084 - val_acc: 0.8213\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.4591 - acc: 0.8238 - val_loss: 0.5050 - val_acc: 0.8080\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4543 - acc: 0.8259 - val_loss: 0.4874 - val_acc: 0.8360\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4519 - acc: 0.8339 - val_loss: 0.4858 - val_acc: 0.8427\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 0.4522 - acc: 0.8318 - val_loss: 0.5078 - val_acc: 0.8267\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 0.4462 - acc: 0.8311 - val_loss: 0.4730 - val_acc: 0.8440\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 0.4446 - acc: 0.8332 - val_loss: 0.5578 - val_acc: 0.8027\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.4471 - acc: 0.8308 - val_loss: 0.5255 - val_acc: 0.8133\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4446 - acc: 0.8332 - val_loss: 0.5020 - val_acc: 0.8333\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4312 - acc: 0.8344 - val_loss: 0.4955 - val_acc: 0.8360\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.4311 - acc: 0.8393 - val_loss: 0.5037 - val_acc: 0.8187\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4315 - acc: 0.8336 - val_loss: 0.4786 - val_acc: 0.8360\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4314 - acc: 0.8344 - val_loss: 0.5047 - val_acc: 0.8267\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4192 - acc: 0.8384 - val_loss: 0.4739 - val_acc: 0.8387\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.4343 - acc: 0.8336 - val_loss: 0.5438 - val_acc: 0.8307\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.4140 - acc: 0.8400 - val_loss: 0.5306 - val_acc: 0.8253\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 0.3980 - acc: 0.8452 - val_loss: 0.4795 - val_acc: 0.8387\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3779 - acc: 0.8480 - val_loss: 0.4834 - val_acc: 0.8387\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3690 - acc: 0.8506 - val_loss: 0.4990 - val_acc: 0.8387\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3630 - acc: 0.8551 - val_loss: 0.5035 - val_acc: 0.8320\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3564 - acc: 0.8548 - val_loss: 0.4819 - val_acc: 0.8413\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3621 - acc: 0.8536 - val_loss: 0.4936 - val_acc: 0.8373\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3497 - acc: 0.8567 - val_loss: 0.5187 - val_acc: 0.8360\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3570 - acc: 0.8602 - val_loss: 0.5153 - val_acc: 0.8280\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3526 - acc: 0.8581 - val_loss: 0.5194 - val_acc: 0.8200\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3493 - acc: 0.8605 - val_loss: 0.5137 - val_acc: 0.8387\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 0.3458 - acc: 0.8558 - val_loss: 0.5254 - val_acc: 0.8320\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3361 - acc: 0.8605 - val_loss: 0.5290 - val_acc: 0.8333\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3354 - acc: 0.8640 - val_loss: 0.5166 - val_acc: 0.8360\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3380 - acc: 0.8567 - val_loss: 0.5198 - val_acc: 0.8320\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3404 - acc: 0.8558 - val_loss: 0.5006 - val_acc: 0.8240\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3423 - acc: 0.8600 - val_loss: 0.5410 - val_acc: 0.8307\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3476 - acc: 0.8572 - val_loss: 0.5384 - val_acc: 0.8333\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 0.3325 - acc: 0.8638 - val_loss: 0.5195 - val_acc: 0.8373\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3375 - acc: 0.8649 - val_loss: 0.5347 - val_acc: 0.8253\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3385 - acc: 0.8602 - val_loss: 0.5300 - val_acc: 0.8280\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3327 - acc: 0.8631 - val_loss: 0.5164 - val_acc: 0.8267\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3283 - acc: 0.8616 - val_loss: 0.5122 - val_acc: 0.8373\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3324 - acc: 0.8628 - val_loss: 0.5209 - val_acc: 0.8333\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3320 - acc: 0.8633 - val_loss: 0.5219 - val_acc: 0.8387\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3329 - acc: 0.8654 - val_loss: 0.5125 - val_acc: 0.8373\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3362 - acc: 0.8631 - val_loss: 0.5278 - val_acc: 0.8280\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3354 - acc: 0.8624 - val_loss: 0.5281 - val_acc: 0.8387\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 0.3371 - acc: 0.8626 - val_loss: 0.5395 - val_acc: 0.8200\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3339 - acc: 0.8626 - val_loss: 0.5284 - val_acc: 0.8400\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3386 - acc: 0.8628 - val_loss: 0.5253 - val_acc: 0.8360\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3332 - acc: 0.8631 - val_loss: 0.5354 - val_acc: 0.8360\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3357 - acc: 0.8595 - val_loss: 0.5069 - val_acc: 0.8307\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3355 - acc: 0.8628 - val_loss: 0.5152 - val_acc: 0.8373\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3386 - acc: 0.8609 - val_loss: 0.5045 - val_acc: 0.8373\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3406 - acc: 0.8593 - val_loss: 0.5271 - val_acc: 0.8387\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3312 - acc: 0.8638 - val_loss: 0.5208 - val_acc: 0.8387\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 4s 31ms/step - loss: 0.3347 - acc: 0.8619 - val_loss: 0.5199 - val_acc: 0.8333\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 0.3328 - acc: 0.8612 - val_loss: 0.5298 - val_acc: 0.8307\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3319 - acc: 0.8678 - val_loss: 0.5345 - val_acc: 0.8213\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3399 - acc: 0.8609 - val_loss: 0.5084 - val_acc: 0.8347\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3360 - acc: 0.8628 - val_loss: 0.5260 - val_acc: 0.8333\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3383 - acc: 0.8612 - val_loss: 0.5404 - val_acc: 0.8360\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3279 - acc: 0.8689 - val_loss: 0.5063 - val_acc: 0.8307\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3410 - acc: 0.8598 - val_loss: 0.5301 - val_acc: 0.8387\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3368 - acc: 0.8616 - val_loss: 0.5258 - val_acc: 0.8293\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3331 - acc: 0.8631 - val_loss: 0.5051 - val_acc: 0.8360\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 4s 32ms/step - loss: 0.3386 - acc: 0.8607 - val_loss: 0.5389 - val_acc: 0.8320\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3368 - acc: 0.8612 - val_loss: 0.5183 - val_acc: 0.8293\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3347 - acc: 0.8626 - val_loss: 0.5217 - val_acc: 0.8293\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3325 - acc: 0.8631 - val_loss: 0.5315 - val_acc: 0.8240\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3390 - acc: 0.8572 - val_loss: 0.5356 - val_acc: 0.8293\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3399 - acc: 0.8602 - val_loss: 0.5374 - val_acc: 0.8293\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3351 - acc: 0.8631 - val_loss: 0.5247 - val_acc: 0.8427\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3371 - acc: 0.8576 - val_loss: 0.5268 - val_acc: 0.8280\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3360 - acc: 0.8605 - val_loss: 0.5143 - val_acc: 0.8320\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 4s 32ms/step - loss: 0.3411 - acc: 0.8614 - val_loss: 0.5249 - val_acc: 0.8347\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3297 - acc: 0.8661 - val_loss: 0.5188 - val_acc: 0.8373\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3339 - acc: 0.8609 - val_loss: 0.5379 - val_acc: 0.8280\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3411 - acc: 0.8595 - val_loss: 0.5322 - val_acc: 0.8387\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3332 - acc: 0.8628 - val_loss: 0.5096 - val_acc: 0.8253\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3351 - acc: 0.8656 - val_loss: 0.5093 - val_acc: 0.8360\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3367 - acc: 0.8612 - val_loss: 0.5234 - val_acc: 0.8400\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3358 - acc: 0.8631 - val_loss: 0.5013 - val_acc: 0.8293\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3362 - acc: 0.8595 - val_loss: 0.5102 - val_acc: 0.8333\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 0.3401 - acc: 0.8631 - val_loss: 0.5203 - val_acc: 0.8320\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3359 - acc: 0.8609 - val_loss: 0.5255 - val_acc: 0.8413\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3347 - acc: 0.8626 - val_loss: 0.5271 - val_acc: 0.8307\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 0.3395 - acc: 0.8569 - val_loss: 0.5234 - val_acc: 0.8333\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 0.3404 - acc: 0.8631 - val_loss: 0.5363 - val_acc: 0.8267\n"
     ]
    }
   ],
   "source": [
    "callbacks = myCallback()\n",
    "batch_size=5\n",
    "FAST_RUN = False\n",
    "epochs=20 if FAST_RUN else 100\n",
    "history = model0.fit_generator(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[\n",
    "        ks.callbacks.ReduceLROnPlateau(), \n",
    "        callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9ff593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:23:23.622338Z",
     "iopub.status.busy": "2022-09-25T13:23:23.621307Z",
     "iopub.status.idle": "2022-09-25T13:23:23.858744Z",
     "shell.execute_reply": "2022-09-25T13:23:23.857771Z"
    },
    "papermill": {
     "duration": 0.499131,
     "end_time": "2022-09-25T13:23:23.860927",
     "exception": false,
     "start_time": "2022-09-25T13:23:23.361796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8TUlEQVR4nO3dd3wUdf748dc7m0ZIAiEJNdKkNwlEQFHAs6FYsKAinuKpqD89bHeKd+fJFT3beR6e5YuegidWbKjYRSNYQRGkKB0CBEIJIb3s+/fHZxM2jQSSZSH7fj4eeWRn5rMz75nZnfd8Pp/ZGVFVjDHGhK6wYAdgjDEmuCwRGGNMiLNEYIwxIc4SgTHGhDhLBMYYE+IsERhjTIizRGAalYi8JyJXNHbZYBKR9SJySgDm+5mIXO17PUFEPqxP2YNYTkcRyRURz8HGup95q4h0a+z5mkPLEoHBd5Ao//OKSIHf8IQDmZeqnqGqMxu77OFIRKaISHoN45NEpFhE+tV3Xqo6S1VPa6S4KiUuVd2oqrGqWtYY8zdNjyUCg+8gEauqscBG4Gy/cbPKy4lIePCiPCw9DxwvIl2qjL8EWKqqPwUhJmMOmCUCUysRGSUiGSJyh4hkAs+KSIKIvCMiWSKy2/c6xe89/s0dE0Vkvog85Cu7TkTOOMiyXUQkXUT2isjHIvKYiDxfS9z1ifFvIrLAN78PRSTJb/qvRWSDiOwUkT/Wtn1UNQP4FPh1lUmXA8/VFUeVmCeKyHy/4VNFZKWI7BGR/wDiN+1oEfnUF98OEZklIi190/4HdATe9tXobheRzr4mnHBfmfYiMkdEdonIahG5xm/eU0XkFRF5zrdtlolIWm3boMo6tPC9L8u3/f4kImG+ad1E5HPf+uwQkZd940VE/iUi20UkR0SWHkhNyjQOSwSmLm2BVkAnYBLuM/Osb7gjUAD8Zz/vHwr8DCQBDwD/FRE5iLIvAN8CicBUqh98/dUnxkuBK4HWQCTwOwAR6QM84Zt/e9/yajx4+8z0j0VEegIDffEe6LYqn0cS8DrwJ9y2WAMM9y8C/MMXX2/gKNw2QVV/TeVa3QM1LOIlIMP3/guBe0XkV37Tz/GVaQnMqU/MPo8CLYCuwEhcQrzSN+1vwIdAAm57PuobfxowAujhe+9FwM56Ls80FlW1P/ur+APWA6f4Xo8CioHo/ZQfCOz2G/4MuNr3eiKw2m9aDKBA2wMpizuIlgIxftOfB56v5zrVFOOf/Ib/H/C+7/WfgZf8pjX3bYNTapl3DJADHO8bvgd46yC31Xzf68uBr/3KCe7AfXUt8x0L/FDTPvQNd/Zty3Bc0igD4vym/wOY4Xs9FfjYb1ofoGA/21aBboDHt536+E27FvjM9/o5YDqQUuX9vwJ+AYYBYcH+/Ifqn9UITF2yVLWwfEBEYkTk/3xV/xwgHWgptV+Rkln+QlXzfS9jD7Bse2CX3ziATbUFXM8YM/1e5/vF1N5/3qqax37OUH0xvQpc7qu9TMAd9A5mW5WrGoP6D4tIGxF5SUQ2++b7PK7mUB/l23Kv37gNQAe/4arbJlrq7h9KAiJ886ppvrfjEtq3vuam3/jW7VNcjeMxYLuITBeR+Hqui2kklghMXarenvY2oCcwVFXjcdV68GvDDoCtQCsRifEbd9R+yjckxq3+8/YtM7GO98zENWmcCsQBbzcwjqoxCJXX917cfunvm+9lVea5v1sKb8Ftyzi/cR2BzXXEVJcdQAmuGazafFU1U1WvUdX2uJrC4+K77FRVp6nqYFztowfw+wbGYg6QJQJzoOJwbd3ZItIKuDvQC1TVDcBCYKqIRIrIccDZAYpxNnCWiJwgIpHAX6n7e/IFkI1r+nhJVYsbGMe7QF8ROd93Jj4Z10RWLg7IBfaISAeqHzi34drpq1HVTcCXwD9EJFpEBgBX4WoVB03dpamvAPeISJyIdAJuLZ+viIzz6yjfjUtWXhE5VkSGikgEkAcUAt6GxGIOnCUCc6AeAZrhzgC/Bt4/RMudAByHa6b5O/AyUFRL2Uc4yBhVdRlwA66zdyvuoJVRx3sU1xzUyfe/QXGo6g5gHHAfbn27Awv8ivwFGATswSWN16vM4h/An0QkW0R+V8MixuP6DbYAbwB3q+rH9YmtDr/FHczXAvNx2/AZ37RjgW9EJBfXAX2Tqq4F4oGncNt5A259H2yEWMwBEF+HjTFHFN/lhytVNeA1EmOaOqsRmCOCrwnhaBEJE5HRwLnAm0EOy5gmwX4pao4UbXFNIIm4pprrVfWH4IZkTNNgTUPGGBPirGnIGGNC3BHXNJSUlKSdO3cOdhjGGHNEWbRo0Q5VTa5p2hGXCDp37szChQuDHYYxxhxRRGRDbdOsacgYY0KcJQJjjAlxlgiMMSbEHXF9BMaYQ6+kpISMjAwKCwvrLmyCKjo6mpSUFCIiIur9HksExpg6ZWRkEBcXR+fOnan9uUIm2FSVnTt3kpGRQZcuVZ+gWjtrGjLG1KmwsJDExERLAoc5ESExMfGAa26WCIwx9WJJ4MhwMPspZBLBT9t/4q5P7yIrLyvYoRhjzGElZBLByh0r+fsXfyczN7PuwsaYw8rOnTsZOHAgAwcOpG3btnTo0KFiuLi4eL/vXbhwIZMnT65zGccff3yjxPrZZ59x1llnNcq8DpWQ6SyO8kQBUFRW27NMjDGHq8TERBYvXgzA1KlTiY2N5Xe/2/fMndLSUsLDaz6cpaWlkZaWVucyvvzyy0aJ9UgUMjWCSE8kAMVl+z97MMYcGSZOnMh1113H0KFDuf322/n222857rjjSE1N5fjjj+fnn38GKp+hT506ld/85jeMGjWKrl27Mm3atIr5xcbGVpQfNWoUF154Ib169WLChAmU36V57ty59OrVi8GDBzN58uQ6z/x37drF2LFjGTBgAMOGDWPJkiUAfP755xU1mtTUVPbu3cvWrVsZMWIEAwcOpF+/fnzxxReNvs1qEzo1gnBfjaDUagTGNMTN79/M4szFjTrPgW0H8sjoRw74fRkZGXz55Zd4PB5ycnL44osvCA8P5+OPP+YPf/gDr732WrX3rFy5knnz5rF371569uzJ9ddfX+2a+x9++IFly5bRvn17hg8fzoIFC0hLS+Paa68lPT2dLl26MH78+Drju/vuu0lNTeXNN9/k008/5fLLL2fx4sU89NBDPPbYYwwfPpzc3Fyio6OZPn06p59+On/84x8pKysjPz//gLfHwQqdRGBNQ8Y0OePGjcPj8QCwZ88errjiClatWoWIUFJSUuN7xowZQ1RUFFFRUbRu3Zpt27aRkpJSqcyQIUMqxg0cOJD169cTGxtL165dK67PHz9+PNOnT99vfPPnz69IRr/61a/YuXMnOTk5DB8+nFtvvZUJEyZw/vnnk5KSwrHHHstvfvMbSkpKGDt2LAMHDmzIpjkgoZMIrEZgTKM4mDP3QGnevHnF67vuuouTTjqJN954g/Xr1zNq1Kga3xMVFVXx2uPxUFpaelBlGmLKlCmMGTOGuXPnMnz4cD744ANGjBhBeno67777LhMnTuTWW2/l8ssvb9Tl1iZk+gisRmBM07Znzx46dOgAwIwZMxp9/j179mTt2rWsX78egJdffrnO95x44onMmjULcH0PSUlJxMfHs2bNGvr3788dd9zBsccey8qVK9mwYQNt2rThmmuu4eqrr+b7779v9HWoTcgkAussNqZpu/3227nzzjtJTU1t9DN4gGbNmvH4448zevRoBg8eTFxcHC1atNjve6ZOncqiRYsYMGAAU6ZMYebMmQA88sgj9OvXjwEDBhAREcEZZ5zBZ599xjHHHENqaiovv/wyN910U6OvQ22OuGcWp6Wl6cE8mGbjno10eqQTT5/9NFcNuioAkRnTdK1YsYLevXsHO4ygy83NJTY2FlXlhhtuoHv37txyyy3BDquamvaXiCxS1Rqvow2ZGoE1DRljGuqpp55i4MCB9O3blz179nDttdcGO6RGYZ3FxhhTT7fccsthWQNoqIDVCETkGRHZLiI/1TK9hYi8LSI/isgyEbkyULGA1QiMMaY2gWwamgGM3s/0G4DlqnoMMAr4p4hEBioY6yw2xpiaBSwRqGo6sGt/RYA4cfdMjfWVbfyufh9PmAePeKxpyBhjqghmZ/F/gN7AFmApcJOqemsqKCKTRGShiCzMyjr420hHhUdZ05AxxlQRzERwOrAYaA8MBP4jIvE1FVTV6aqapqppycnJB73AKE+U1QiMCRHlN5HbsmULF154YY1lRo0aRV2Xoz/yyCOV7vtz5plnkp2d3eD4pk6dykMPPdTg+TSGYCaCK4HX1VkNrAN6BXKBkZ5IqxEYE2Lat2/P7NmzD/r9VRPB3LlzadmyZSNEdvgIZiLYCJwMICJtgJ7A2kAuMCo8yjqLjTkCTZkyhccee6xiuPxsOjc3l5NPPplBgwbRv39/3nrrrWrvXb9+Pf369QOgoKCASy65hN69e3PeeedRUFBQUe76668nLS2Nvn37cvfddwMwbdo0tmzZwkknncRJJ50EQOfOndmxYwcADz/8MP369aNfv3488sgjFcvr3bs311xzDX379uW0006rtJyaLF68mGHDhjFgwADOO+88du/eXbH8Pn36MGDAAC655BKg5ltYN1TAfkcgIi/irgZKEpEM4G4gAkBVnwT+BswQkaWAAHeo6o5AxQO+piGrERjTIDffDL5nxDSagQPBdxyt0cUXX8zNN9/MDTfcAMArr7zCBx98QHR0NG+88Qbx8fHs2LGDYcOGcc4559T63N4nnniCmJgYVqxYwZIlSxg0aFDFtHvuuYdWrVpRVlbGySefzJIlS5g8eTIPP/ww8+bNIykpqdK8Fi1axLPPPss333yDqjJ06FBGjhxJQkICq1at4sUXX+Spp57ioosu4rXXXuOyyy6rdf0uv/xyHn30UUaOHMmf//xn/vKXv/DII49w3333sW7dOqKioiqao2q6hXVDBfKqofGq2k5VI1Q1RVX/q6pP+pIAqrpFVU9T1f6q2k9Vnw9ULOWiwq2PwJgjUWpqKtu3b2fLli38+OOPJCQkcNRRR6Gq/OEPf2DAgAGccsopbN68mW3bttU6n/T09IoD8oABAxgwYEDFtFdeeYVBgwaRmprKsmXLWL58+X5jmj9/Pueddx7NmzcnNjaW888/v+JhMl26dKm4jfTgwYMrblRXkz179pCdnc3IkSMBuOKKK0hPT6+IccKECTz//PMVT2Arv4X1tGnTyM7OrvXJbAciZH5ZDFYjMKYx7O/MPZDGjRvH7NmzyczM5OKLLwZg1qxZZGVlsWjRIiIiIujcuTOFhYUHPO9169bx0EMP8d1335GQkMDEiRMPaj7lqt7Guq6modq8++67pKen8/bbb3PPPfewdOnSGm9h3atXw7pXQ+ZeQ+DrLLYagTFHpIsvvpiXXnqJ2bNnM27cOMCdTbdu3ZqIiAjmzZvHhg0b9juPESNG8MILLwDw008/VTw6Micnh+bNm9OiRQu2bdvGe++9V/GeuLi4GtvhTzzxRN58803y8/PJy8vjjTfe4MQTTzzg9WrRogUJCQkVtYn//e9/jBw5Eq/Xy6ZNmzjppJO4//772bNnD7m5uTXewrqhQqtGYJ3Fxhyx+vbty969e+nQoQPt2rUDYMKECZx99tn079+ftLS0Os+Mr7/+eq688kp69+5N7969GTx4MEDF7Z979erFUUcdxfDhwyveM2nSJEaPHk379u2ZN29exfhBgwYxceJEhgwZAsDVV19NamrqfpuBajNz5kyuu+468vPz6dq1K88++yxlZWVcdtll7NmzB1Vl8uTJtGzZkrvuuot58+YRFhZG3759OeOMMw54eVWFzG2oAUY/P5rdhbv55upvGjkqY5o2uw31kcVuQ70f1llsjDHVhVYisM5iY4ypJqQSgXUWG3PwjrRm5FB1MPsppBJBlMc6i405GNHR0ezcudOSwWFOVdm5c+cB/8gs5K4asqYhYw5cSkoKGRkZNOTuv+bQiI6OJiUl5YDeE1qJwO4+asxBiYiIoEuXLsEOwwRIaDUNWY3AGGOqCalEEOmJpLis2No5jTHGT0glgvIH2Jd4S4IciTHGHD5CKxGEu0Rg/QTGGLNPaCUCX43A+gmMMWaf0EoEViMwxphqApYIROQZEdkuIj/tp8woEVksIstE5PNAxVIu0hMJWI3AGGP8BbJGMAMYXdtEEWkJPA6co6p9gXEBjAXY1zRkvy42xph9AvmoynRg136KXAq8rqobfeW3ByqWctY0ZIwx1QWzj6AHkCAin4nIIhG5vLaCIjJJRBaKyMKG/MTdOouNMaa6YCaCcGAwMAY4HbhLRHrUVFBVp6tqmqqmJScnH/QCK/oIrEZgjDEVgnmvoQxgp6rmAXkikg4cA/wSqAVWNA1ZjcAYYyoEs0bwFnCCiISLSAwwFFgRyAVaZ7ExxlQXsBqBiLwIjAKSRCQDuBuIAFDVJ1V1hYi8DywBvMDTqlrrpaaNwTqLjTGmuoAlAlUdX48yDwIPBiqGqqyz2BhjqgupXxZbZ7ExxlQXUomgvGnI+giMMWaf0EoE1jRkjDHVhFYisM5iY4ypJrQSgdUIjDGmmpBKBBGeCMBqBMYY4y+kEkGYhBERFmGdxcYY4yekEgG4fgJrGjLGmH1CLxF4oqxpyBhj/IReIrAagTHGVBJyiSDSE2mJwBhj/IRcIojyRFlnsTHG+Am9RBBufQTGGOMv9BKBx/oIjDHGX+glAqsRGGNMJSGXCKyz2BhjKgtYIhCRZ0Rku4js96ljInKsiJSKyIWBisWfdRYbY0xlgawRzABG76+AiHiA+4EPAxhHJdY0ZIwxlQUsEahqOrCrjmK/BV4Dtgcqjqqss9gYYyoLWh+BiHQAzgOeqEfZSSKyUEQWZmVlNWi5ViMwxpjKgtlZ/Ahwh6p66yqoqtNVNU1V05KTkxu00Mgw6yw2xhh/4UFcdhrwkogAJAFnikipqr4ZyIVGhVtnsTHG+AtaIlDVLuWvRWQG8E6gkwDY3UeNMaaqgCUCEXkRGAUkiUgGcDcQAaCqTwZquXWxu48aY0xlAUsEqjr+AMpODFQcVUV6Iin1luJVL2EScr+nM8aYakLuSFjxAHtrHjLGGCAUE0G4SwTWYWyMMU7oJYLyGoH1ExhjDBCKiSDcmoaMMcZfyCWCSE8kYDUCY4wpF3KJwDqLjTGmstBLBNZZbIwxlYReIrDOYmOMqSRkEsHq1fDYY1CS3xywpiFjjCkXMolg8WK48UbYvS0WsBqBMcaUC5lEEB/v/pfkNwOsRmCMMeVCLxEUWGexMcb4C7lEUJQX7f5b05AxxgAhmAgK830/KLOmIWOMAUIwERTkRQBWIzDGmHIhkwhi3cVCFOT6EoHVCIwxBghgIhCRZ0Rku4j8VMv0CSKyRESWisiXInJMoGIBCAtzySA/1z2LxzqLjTHGCWSNYAYwej/T1wEjVbU/8DdgegBjAVzzUHkisKYhY4xxAvmoynQR6byf6V/6DX4NpAQqlnLx8ZCXKwhiTUPGGONzuPQRXAW8V9tEEZkkIgtFZGFWVtZBLyQ+HnJyhEhPpNUIjDHGJ+iJQEROwiWCO2oro6rTVTVNVdOSk5MPelkuEbg7kFofgTHGOPVKBCLSXETCfK97iMg5IhLR0IWLyADgaeBcVd3Z0PnVJS7Olwg8UdY0ZIwxPvWtEaQD0SLSAfgQ+DWuM/igiUhH4HXg16r6S0PmVV/+NQJrGjLGGKe+ncWiqvkichXwuKo+ICKL9/sGkReBUUCSiGQAdwMRAKr6JPBnIBF4XEQASlU17aDWop7KE0GixxKBMcaUq3ciEJHjgAm49nwAz/7eoKrj65h+NXB1PZffKOLjYe9eaBsWaU1DxhjjU9+moZuBO4E3VHWZiHQF5gUsqgCJj4eyMojUFtZZbIwxPvWqEajq58DnAL5O4x2qOjmQgQVC+f2GPMUJ1jRkjDE+9b1q6AURiReR5sBPwHIR+X1gQ2t8cXHuv6ckwZqGjDHGp75NQ31UNQcYi/vhVxfclUNHlPIaQVhRS6sRGGOMT30TQYTvdwNjgTmqWgJowKIKkPJEIMUtrEZgjDE+9U0E/wesB5oD6SLSCcgJVFCB4p8IrLPYGGOc+nYWTwOm+Y3a4Ls1xBGlPBFQFGdNQ8YY41PfzuIWIvJw+Y3fROSfuNrBEaU8EWhRnDUNGWOMT32bhp4B9gIX+f5ygGcDFVSglCcCb0Gs1QiMMcanvr8sPlpVL/Ab/ktdt5g4HEVFQUQEeAtjrUZgjDE+9a0RFIjICeUDIjIcKAhMSIEVHw9lhc2ts9gYY3zqWyO4DnhORFr4hncDVwQmpMCKj4fSghhrGjLGGJ/6XjX0I3CMiMT7hnNE5GZgSQBjC4j4eCgqaIZXvZR6SwkPC9jTOo0x5ohwQE8oU9Uc3y+MAW4NQDwBFx8PxQXRANZPYIwxNOxRldJoURxC8fFQnO9LBNY8ZIwxDUoER9wtJsDdeK44LwrAOoyNMYY6EoGI7BWRnBr+9gLt63jvMyKyXUR+qmW6iMg0EVktIktEZFAD1qPe4uOhMD8SsKYhY4yBOhKBqsapanwNf3GqWlcv6wxg9H6mnwF09/1NAp44kMAPVnw8FOb5EoE1DRljTIOahvZLVdOBXfspci7wnDpfAy1FpF2g4ikXHw/FheFQ5rEagTHGEMBEUA8dgE1+wxm+cdWIyKTy+xxlZWU1aKEVN54rthvPGWMMBDcR1JuqTlfVNFVNS05ObtC89t2BNN46i40xhuAmgs3AUX7DKb5xAVX+uEqK4tldsDvQizPGmMNeMBPBHOBy39VDw4A9qro10Av1rxFszQ344owx5rAXsPsriMiLwCggSUQygLuBCABVfRKYC5wJrAbygSsDFYs//0SwZe+WQ7FIY4w5rAUsEajq+DqmK3BDoJZfm/JEEKcplgiMMYYjpLO4MZUngnjpYInAGGMI4UQQq+0tERhjDCGYCGJj3f9mZW0sERhjDCGYCMLCXDKIKk1iW942Sr2lwQ7JGGOCKuQSAbjmIU9JK7zqZXve9mCHY4wxQRWyiYBi11lgzUPGmFAXsonAW+A6CywRGGNCXcgmgtJC95QySwTGmFAXkokgLg4K8iIRxBKBMSbkhWQiiI+HvTlCm1i7hNQYY0I2EeTkQPs4+1GZMcaEdCJoF2uJwBhjQjYReL3QOqKzJQJjTMgL2UQA0MrTiaz8LHtSmTEmpIVkIih/SllLOgKQmZsZxGiMMSa4QjIR+N+BFOy3BMaY0BbQRCAio0XkZxFZLSJTapjeUUTmicgPIrJERM4MZDzlWrf2vdjbDrBEYIwJbQFLBCLiAR4DzgD6AONFpE+VYn8CXlHVVOAS4PFAxeOvd2/3f8cGlxEsERhjQlkgawRDgNWqulZVi4GXgHOrlFGg/CnCLYBDckSOj4eUFNiwJhaPeCwRGGNCWiATQQdgk99whm+cv6nAZb6H288FflvTjERkkogsFJGFWVlZjRJcnz6wfJnQLq6dJQJjTEgLdmfxeGCGqqYAZwL/E5FqManqdFVNU9W05OTkRllw376wYgW0a27PLjbGhLZAJoLNwFF+wym+cf6uAl4BUNWvgGggKYAxVejTBwoKoGXRMZYIjDEhLZCJ4Dugu4h0EZFIXGfwnCplNgInA4hIb1wiaJy2nzr07ev+R+wYaInAGBPSApYIVLUUuBH4AFiBuzpomYj8VUTO8RW7DbhGRH4EXgQmqqoGKiZ/5VcOlW3rwe7C3RSWFh6KxRpjzGEnPJAzV9W5uE5g/3F/9nu9HBgeyBhq07IltG8Pezd3gnawde9WuiR0CUYoxhgTVMHuLA6qvn0ha4PrfLbmIWNMqArpRNCnD2xeEwdeYdWuVcEOxxhjgiLkE0F+fhhxhf1YsHFBsMMxxpigCOlEUH7lUC89n/mb5gc3GGOMCZKQTgR9fHc+SsobwcodK8nKOyRXrhpjzGElpBNBQgK0awdkuarBl5u+DG5AxhgTBCGdCMDVCrLWJxPpiWT+RmseMsaEHksEfWDlijDS2h1r/QTGmJAU8omgb1/IzYVjmo1h0ZZF5JfkBzskY4w5pEI+EZR3GLfKPoUSbwnfbf4uuAEZY8whFvKJYMgQSEyEJR8dA8AXG78IckTGGHNohXwiiIqCiRPhvXci6Rk10jqMjTEhJ+QTAcCkSVBaCi1W3MyXm76kzFsW7JCMMeaQsUQA9OgBo0bBuk9OZm9hLku3Lw12SMYYc8hYIvC59lrI2hwHa0/hvVXvBTscY4w5ZCwR+Jx3HiQlQdLKP/L0D0/jVW+wQzLGmEMioIlAREaLyM8islpEptRS5iIRWS4iy0TkhUDGsz/lnca7F5/I2o15fLz242CFYowxh1TAEoGIeIDHgDOAPsB4EelTpUx34E5guKr2BW4OVDz1MWkSqFeIePc5nvzmv8EMxRhjDplA1giGAKtVda2qFgMvAedWKXMN8Jiq7gZQ1e0BjKdO3bvD448LJStP480HzmFLTmYwwzHGmEMikImgA7DJbzjDN85fD6CHiCwQka9FZHRNMxKRSSKyUEQWZmUF9lbR114LN/9hB/rjBMZdvRHVgC7OGGOCLqAPr6/n8rsDo4AUIF1E+qtqtn8hVZ0OTAdIS0sL+KH54b8nMfu71/jy1Qvotkjp3Vvo1Qs6dHC/Qk5MhBNOgBYtAh2JMcYEXiATwWbgKL/hFN84fxnAN6paAqwTkV9wiSGoN/wRgQceKuXSOybj3XsZ3//chY8+bkVxkaeizCWXwIsvBjFIY4xpJIFsGvoO6C4iXUQkErgEmFOlzJu42gAikoRrKlobwJjq7YK+53HGZavZfuZJbL2sNcV3RHDSk+fzyyovl18Ob74Je/c23vJUYefOxpufMcbUV8ASgaqWAjcCHwArgFdUdZmI/FVEzvEV+wDYKSLLgXnA71X1sDgcRnoimTthLrl35rLz9p08ePoDzMt8g/d3PsZ110FhoUsGjWXWLGjbFr4LYF0oPR2+sHvqGWOqED3CekPT0tJ04cKFh3y5qspZL57Fp+s+ZeE1izhraB969YL3GulHyCeeCPPnw3HHwYIFrnmqMalCly6QlQXffuuew2CMCR0iskhV02qaZr8sricR4b/n/JfYyFh+/cZlXHxJGR99BNsb4YLX1atdEkhLg6++Ckzfw/LlsGED5OfDuHHuYTyNbc8eGD8eZs6EsiZy377PP3f7Iz0d1qw5PNZr40YoKAh2FHVTha+/dp+Lg1FWBnl5jRvTwToczpdVA/f5s0RwANrGtuWps5/ih8wf+DBmImVl8OTMXQ2e74wZEBYGb7wBgwfD7bc3/hdg7lz3/7nn4Oef3WWyjf3hfvFFeOkl9wvtgQPhnXcOjy/QwVq+HE45BS69FEaOhG7d3PMr1vr1YpWVwVNPwa23ugS4ZAmUlDR+LKrw0Udw6qnQqRN07Ah3313zicjixXDjjfD4440fR14efPkl/Oc/8Ic/wOaql3/4/PwznHmmq+Gec071A5gqePdzFxevF8aOhZQU+Hg/P/IvKoJPPoFp0+C66+Dkk2HAAHeFX6tWcMstsG3bAa9mhVWr4NhjoXVrF8+DD7oTt0Pt00/dtnzqqQAtQFWPqL/BgwdrsP3rq39pj0d7KG0WKykL9NLXLtXi0uJKZYqKVJcsUf3wQ9XnnlOdP7/meZWWqqakqJ5xhhueP18VVO+6q3FjHjlS9Zhj3Ou//90t4z//adxlnHiiau/eqq+8otqtm1vG9OmNu4wDtXGj6n33qU6cqHr88apdu6reeafqjh37f5/X67ZZQoLq11+7/fjoo6otW6q2aKH65puqCxeqpqW59YyMdP9BtU0b1SefVC0pqTs+r9d9BqpKT1c97TTVIUPcMo4+2s27XTvVqVNVzz7bDUdFqQ4frjphguqUKarDhrnxIu7///5Xv+308ceqkyapPvWU6po1Lq5ypaWqc+eqjh2r6vHsW09Q7dVLdfv2fWULClRvv101IkI1Pl718stdub/+dV+ZTZtUBwxQbd7crd9vfuPm7++f/9y3LT0e1ccfrzz9559Vb7tNNTFxXywJCW79zz3XzfOii1TDwlRjYlzZ++5z/6+6yu0//3UsLlZ95x3VTz5Rzc93415/3a1DYqLqr3+97zMdG6v60Ue1b8vCQve5q49du9xn6/77VX/3Oxf3hReq3nSTW+fXX1c95RS33JQU1Zdfrt98awIs1FqOq0E/sB/o3+GQCMrddtd29yGc3EXPf/n8imQwb96+L275X1iY6gcfVJ/HRx+56f47ePx41ehod+Ap/1A2RHa2+zLdeacbLitTHTPGHSwa60C9YYNbj7//3Q0XF7svZceO7nVj+fRT1RtucF+c2bNV339f9ZFHVK+91n2B/vEPdxBdskT1yivdAQlUO3RQHTXKJVwR1bg4d8D6619dudGjXQIr97//uff93/9VXv7ataqDB+/bp23bqr74ojtYrlihOmuWOzCDS4rPP6+6apXb5kVFqnPmqF58sWrr1u4AJeJiPP98dyDats0dqMq/+Kef7mI+5xzV//7XHWTKrVypOnmyS1idOrl93LOn6r/+pZqZ6dY3IkL1s8/2vSc7WzUrq/I6zZqlGh5eOZm1bu0OfAMGuDhANTnZHUjnzHEH888/d5/T1FTV3bvdNu/Xz5X9zW/cuni9qpde6mJbsEB13TrVLl3c9v9//0/15JNVk5Lce/72N1d+4UIX99ixqnv2qJ51lpt+6qkumbdp44bDw1UvuED17bdVt26tfGD330bjx+9LjNHRLpmD24+vvaZ677371rE8qQ8a5F4fe6z7bPvv//79XXxVD8per+qrr6p27uximzGj5s/w5s2qf/mL21f+x4hmzVTbt3fjmzffNz4pye3TgoKa51dflggCZP16twUHnvyzMu4CHXXP7/Xa60oVXCKYOdMdlJYtcx+ehATV1asrz+PSS90H038nb9nizq7Lz4juu899iTIza/6wV7VunWpe3r7hV1918/KvleTnq555phv/r381ZCs4993n5rVmzb5x777rxj3zTMPnXz6/yMjKByz/s8GuXSuPi45WvfFGt5/8/fST6rhx+8q1b+++vKB6883uDLd1a3e2WlZWPY6CAndAvOUWdwCsyut1Z3Ldu1f+kpcfgBIT3Znybbe5mt+NN7qDbPmZvMfjklRu7oFtn9LSyp+PXbvcGXtCgqv9nXWWO4B5PC4Zffml6rRpbrkjR7oksXy5K3vVVe4Aeu657mD78ssukVX13ntunr17u9pJmzZunL89e9zBv2NHd8BNSFD99tvK2/Oyy1wc48e7BJSSorpz5771mjLF7d9Ro1xsDz/sDv71tXOn6t69bvuUlKg++6yLqXz/nHqq6ltvuc/Y73/vEs4tt1ROvOV271Y94QS3r377W3fyc999bhuCS4YjRrjX997rllle47jgApckwCXBe+91tbGqnyOv1yXb9HTVnJz6r+f+WCIIoPPOq3JQkjI9/qIv9bNfvtXSsn11/jVr3Begf3/3gVR1X7zoaNXrr688zzkr5+i4V8bp7LlZFdXC8r8WLdzZcE0HKFXVF15wX8wRI/Y1OUyc6JZdtamiqMh9MMF9oL/6ypXxel1zyK23ql5ySfUP6dy57mx/xYp94wYMcOP8eb3ubLF795qbP1Td2emcOe5s8KKL3Be8pmT3/vvuQDNokDvAZWerfv+9O9v1T5BZWe4L/eij7ox0f7Ky9tW4iorc2TW4A3ZYmJt/QxQXq37zjTuTv+UWdwB7552aa0hFRapvvOES0eLFDVuuvzVr9iWZo45y+/TWW93nqPwzNXZsw842X33VJZezz67cTOTvq69cmaSkmtfP61W95x6tqGmlpx98PPVVXOw+K8uWHfh78/NdDdT/u5mcrPrEE+47VFTkTvLANe2U74PERNcEtGpV469PXSwRBNiePe7DfeNDn2qfu8arTBVlKtrxXx316UVPa0mZOwJ/8IH7kA8a5M4o2rZ1e6D87GhD9gYd+9JYZSrKVHTMrDHq9Xp19Wp3APn3v/dVk3/1K3fGUM7rVX3wQTetvC3z3ntdwmjTxp1p1aSkRPXqq/d9mOPjXTMKuIQSHq46dKhbR1XX7BUd7ab37OkOyEuXuuFp06rP/7XX3LQXX6y+3Icfds0j5ctu1879nzhx38GyrEz1pZfcMgcO3HeWGCgvvOCq5bfdFtjlHEpr17qE5H/ysHevO/P/61/r15dRl927666tLlhQvXZW1QcfuM/MkaKszH1WCwqqn+yUlbmaXUKCq4HOmVNzrepQsURwiGXlZekLS17QIU8NUaai3aZ10+cWP6fFpcX6xBPuQD1ihOoVV6g+9XSpfrT6Y736rau1+T3Ntdnfm+n98+/XB+Y/oExFn1v8XKV5e73uDLN5c/cBGzvWtXGff77bm+PGuQ/luHHuIP7EE1qvTsPt2131f9IkN6+ZM92X+8033XyOP951pMXGqvbp476s4eHuLHDKFHe2l5lZfb5lZa58v37udW6ua1sub2cfM0b1iy9c9dfrdR2h5VX1mTP3tTkPHFi9bTtQ8vPr1wRnzJHEEkGQeL1efWvlWzrgiQHKVLTDPzvofV/cpz9s/UGnL5yul71+mbZ+sLUyFY29N1Z//fqvdd3udaqqWlpWqsP/O1wT7kvQLTlbqs37l19cB2K/fu4MvkULdxZbfta3c+e+DjCR2qvs9TF79r6rRbp2dZ1dqvvalz0e16lZm+efd+XKa0DlfR8vv1zzAffZZ/e1o/bt697fGGetxoSy/SUC+2XxIeBVL++vfp+Hv3qYT9Z9UjG+TfM2jOo8inF9xnFm9zNpFtGs0vt+2fkLxzx5DKcdfRpvXvwmcoA/N543z11XPXSo+6FaQ8yeDY89Bs88436hDO6QfuWV7vr5mTPh8strfm9pKVxzjbu+vlcv6NnTXQ/fsmXty/vqK9i1C844w/3GwhjTMPv7ZbElgkNsybYl/Jj5I0NThtK9Vfc6D+4Pf/Uwt314G51bdubsHmdzVo+zOC7lOOKi4uq1vJdecj8+Ov74xoi+usJC+OADOOss8HjqLm+MCQ5LBEcwr3p57sfneH3F63y89mMKSgsQhO6J3Ultm0qXll1oF9eOdrHtiPBEUOYto0zL6JrQlYFtBxIeFuxHThhjDgeWCJqI/JJ80jeks3DLQr7f+j0/ZP5ARk4Gpd7SGsvHRsYy/Kjh9ErqRUJ0Ai2jW1LiLWHr3q1szd2KV720i21Hu7h2DGo3iJO7nHzAzU/GmCODJYImzKtedubvJDM3kzItwyMeRIRl25eRviGd9I3pbMjewN7ifQ9PaBbejHZx7fCIhy17t5BX4m5sNLLTSB449QGGdBhy0PHkl+QTExHT4PUyxjQuSwSGUm8p2YXZRIRFEB8VX+nMP6coh+eXPM/Uz6aSlZ/FCR1PIDYyFnBXlZVpGaXeUrzq7hImCHFRcUwaNImzepyFiJBdmM0fP/kjTy56khuOvYGHT3/YmqWMOYxYIjD1srdoL//86p+8v/p9lH2fi/CwcDziIUzc5TuKsm73OjblbCK1bSoX9rmQf3/zb3bk72Bkp5HMWz+PU7qewssXvkyrZq2CtTrGGD9BSwQiMhr4N+ABnlbV+2opdwEwGzhWVfd7lLdEcHgoKSth1tJZ3PPFPazetZohHYbwxJgnGNRuEM/88AzXvXMdnVp24tVxrzKw7cBgh2tMyAtKIhARD/ALcCruIfXfAeNVdXmVcnHAu0AkcKMlgiNLqbeUlTtW0ie5T0WNAWDBxgVc+OqF7MjfwR3D7+BPI/5EdHh0ECM1JrQF6wllQ4DVqrpWVYuBl4Bzayj3N+B+oDCAsZgACQ8Lp1/rfpWSAMDwjsNZ9v+WMaH/BO754h5S/y+V77d+H6QojTH7E8hE0AHY5Dec4RtXQUQGAUep6rv7m5GITBKRhSKyMCsrq/EjNQHRqlkrZoydwXsT3iO3OJfhzwznxaUBeA6nMaZBgvbjfREJAx4GbqurrKpOV9U0VU1LTk4OfHCmUY3uNppFkxZxbPtjufT1S5ny8RTKvIfBw3+NMUBgE8Fm4Ci/4RTfuHJxQD/gMxFZDwwD5ohIjW1Y5sjWunlrPr78Y64bfB33L7if7o9258/z/swvO38JdmjGhLxAdhaH4zqLT8YlgO+AS1V1WS3lPwN+Z53FTd9ry1/jyUVP8snaT1CUPsl9GNFxBCM6jWBU51G0i2tXqXxxWTGqSlR4VJAiNubIt7/O4oD94kdVS0XkRuAD3OWjz6jqMhH5K+52qHMCtWxzeLugzwVc0OcCNuds5uVlL/Px2o+ZtXQWTy56EoC+yX05pespRIdHs2DTAr7b/B3NIppxy7BbuGnoTbSIbkFhaSGfrf+MNbvW0Ce5D/3b9CcpJinIa2bMkcl+UGYOC6XeUn7M/JFP133KR2s/4ouNX1DqLWVwu8EMP2o4a7PX8ubKN2kZ3ZJhKcNI35BOfkl+pXm0jW3L0QlH0zWhK0cnHE23Vt3o1qobPRJ7kNAsoUHxFZYW8mPmj3Rq2Ym2sW0bNC9jgsF+WWyOOEWlRXjVW+kZDT9s/YG/fP4XVuxYwaldT2VM9zH0a92PlTtWsmTbEpZnLWdt9lrW7FpDRk5Gxa+jBeH0bqdzderVnN3zbCI9kZWW5VUvO/J3sGXvFrbu3UpmbiZZ+VnszN9JVn4WS7cvZXHmYkq9pTSPaM7UUVO5aehNRHgiAHcbjpyiHPJL8skvySc2MpY2sW0O3cbyxZCRk0FsZGy9kp6qsi1vGxk5GezI30FWXhaRnkjG9BhTcXuR+sotzmX+xvkkNkukV1KvilukF5UWsSN/B21j2+IJ23ePcq96efvnt9lVsIu+rfvSJ7lPjcvcU7iH7XnbadWsFQnNEqpdolxVYWkhS7ctZW/xXvKK8yj1ltIrqRc9EntUWn5VecV5/JD5A4szF9O/dX9GdBpR480Xs/Ky+HDNh5RpGWO6jyExJrG+m6iS7MJsft7xM5tyNtE1oSu9k3pXexYJwK6CXSzZtoRdBbsoKCmgoLSA/q37MzRl6EEt1xKBCTlFpUWs3b2W1btW8+3mb5n540w25WwiITqBdnHtiA6PJiIsgu1529m8dzPFZcXV5hHpiaw4uA3tMJTUdqk8v+R53v7lbfom9+WMbmfwfeb3LNqyiD1Feyq9t2tCV07oeALHpRzHMW2OoX+b/jSPaE5mbibLspaxIXsDIlJx647C0kIKSwvJLc5lffZ61mavZdOeTYRJGFHhUUR6IsktzmVXwS52F+ymVbNWdEnoQueWncnMzeSHrT+wu3A3AL2SenFcynG0iGrBltwtbM7ZTG5xLp4wDx7xsLd4L+uz11NYWv2nO80jmnNR34sY030MW3O3smrnKrbmbqVFVAsSYxJJbJZI88jmxETEUFJWwtzVc3l/9fuV5tUuth1FZUXsKtgFQMcWHbk+7XquSr2KRVsXcecnd7I4c3Gl5R4VfxQ9k3rSo1UPSrwlfJXxFcu2L6uUzFs1a0Vy82RaN2/t/mJa0ya2DWESxmfrP2PBpgU1rlNMRAx9k/tSXFbMjvwd7CrYRXhYODERMUSHR7MpZ1PFfbQAhnQYwh3D76Bbq278tP0nlm5bymcbPuObjG8q4vGIh5O6nERq21R2FeyqmG92YTbZhdkUlBYQHR5NdHg0kZ5ISr2llHpLyS3OZXve9krxCUKXhC4kNkskLiqOKE8UK3asYH32+mrrcvvxt3P/qfdXG18flghMyCvzlvHR2o+YvXw2e4r2UFhaSHFZMckxyaTEp9AhrgPt49pXPNshuXkyzSOa13hmOOfnOUx+bzJb9m7hmLbHkNYuje6J3YmJiCEmIoasvCwWbFrA/I3zycp3v3spv1FfTlFOnbEmxSTRNaErHVt0RBCKyoooLC0kLjKOVs1a0TK6JTvyd7Auex3rs9eTHJNMattUjml7DNmF2XyV8RVfZ3xNQUkBHeLdesVHxVPmLcOrXmIiYujcsjNdWnYhJT6F5ObJJMckk5mbyYzFM3h52csVd6SNjYylfVx7copy2Jm/kxJvSaVY28e15/xe53N2z7PJK85j5Y6VrNq1ipiIGNrGtqVFVAve+vktPln3CR7xUKZldGnZhb+d9DeGdBjC8qzl/LT9J1buXMkvO3/h5x0/IyIMSxnGcSnH0bllZ3YX7GZXwS6y8rPcX14W2/O2sy1vW0Wy6d+6P6d0PYUTO55IYkwiMRExCMKyrGV8v/V7lmUtIyYihqRmSbRq1opSbyl5JXnkl+TTNaErx7Y/lgFtBvDe6vd48MsHWbt7bcU6hoeFM6jdIMZ0H8OZ3c8E4PUVr/P6itdZu3stSTFJJMUkkRiTSMvolrSMbkmz8GYUlRZRUFpAcVkxEZ4IwsPCaRbejG6tutErqRcp8Sms272uYv2zC7PJLc4lvySfbq26MajtIFLbpdKmeRuaRTSjWXgzEpolHHCNrZwlAmMamVe9lHpLqzUz+VNVNuzZwJJtS1iybQmZuZn0TOxJ39Z9OTrhaESk4q6u5WeP5ckkmHKLc1m2fRmdWnaiTfM2FclQVSsOVPkl+ZR6Szm61dF1NtkALM9azozFM+jSsgtXDbqq1u1Wfjyq73MxisuKKSwtJD4qvp5rV7cybxnv/PIO+SX59G/Tnx6JPfYb75HyDA9LBMYYE+KCda8hY4wxRwBLBMYYE+IsERhjTIizRGCMMSHOEoExxoQ4SwTGGBPiLBEYY0yIs0RgjDEh7oj7QZmIZAEbDuAtScCOAIVzOAvF9Q7FdYbQXO9QXGdo2Hp3UtUaH/F4xCWCAyUiC2v7NV1TForrHYrrDKG53qG4zhC49bamIWOMCXGWCIwxJsSFQiKYHuwAgiQU1zsU1xlCc71DcZ0hQOvd5PsIjDHG7F8o1AiMMcbshyUCY4wJcU06EYjIaBH5WURWi8iUYMcTCCJylIjME5HlIrJMRG7yjW8lIh+JyCrf/7qfaH4EEhGPiPwgIu/4hruIyDe+ff6yiNT+CLEjkIi0FJHZIrJSRFaIyHGhsK9F5Bbf5/snEXlRRKKb2r4WkWdEZLuI/OQ3rsZ9K84037ovEZFBDVl2k00EIuIBHgPOAPoA40WkT3CjCohS4DZV7QMMA27wrecU4BNV7Q584htuim4CVvgN3w/8S1W7AbuBq4ISVeD8G3hfVXsBx+DWvUnvaxHpAEwG0lS1H+ABLqHp7esZwOgq42rbt2cA3X1/k4AnGrLgJpsIgCHAalVdq6rFwEvAuUGOqdGp6lZV/d73ei/uwNABt64zfcVmAmODEmAAiUgKMAZ42jcswK+A2b4iTWq9RaQFMAL4L4CqFqtqNiGwr4FwoJmIhAMxwFaa2L5W1XRgV5XRte3bc4Hn1PkaaCki7Q522U05EXQANvkNZ/jGNVki0hlIBb4B2qjqVt+kTKBNsOIKoEeA2wGvbzgRyFbVUt9wU9vnXYAs4Flfc9jTItKcJr6vVXUz8BCwEZcA9gCLaNr7ulxt+7ZRj29NORGEFBGJBV4DblbVHP9p6q4RblLXCYvIWcB2VV0U7FgOoXBgEPCEqqYCeVRpBmqi+zoBdwbcBWgPNKd6E0qTF8h925QTwWbgKL/hFN+4JkdEInBJYJaqvu4bva28quj7vz1Y8QXIcOAcEVmPa/b7Fa79vKWv+QCa3j7PADJU9Rvf8GxcYmjq+/oUYJ2qZqlqCfA6bv835X1drrZ926jHt6acCL4DuvuuLIjEdS7NCXJMjc7XLv5fYIWqPuw3aQ5whe/1FcBbhzq2QFLVO1U1RVU74/btp6o6AZgHXOgr1qTWW1UzgU0i0tM36mRgOU18X+OahIaJSIzv816+3k12X/upbd/OAS73XT00DNjj14R04FS1yf4BZwK/AGuAPwY7ngCt4wm46uISYLHv70xce/knwCrgY6BVsGMN4DYYBbzje90V+BZYDbwKRAU7vkZe14HAQt/+fhNICIV9DfwFWAn8BPwPiGpq+xp4EdcHUoKr/V1V274FBHdV5BpgKe6KqoNett1iwhhjQlxTbhoyxhhTD5YIjDEmxFkiMMaYEGeJwBhjQpwlAmOMCXGWCIzxEZEyEVns99doN28Tkc7+d5U05nASXncRY0JGgaoODHYQxhxqViMwpg4isl5EHhCRpSLyrYh0843vLCKf+u4H/4mIdPSNbyMib4jIj76/432z8ojIU7776n8oIs185Sf7niexREReCtJqmhBmicCYfZpVaRq62G/aHlXtD/wHd9dTgEeBmao6AJgFTPONnwZ8rqrH4O4FtMw3vjvwmKr2BbKBC3zjpwCpvvlcF5hVM6Z29stiY3xEJFdVY2sYvx74laqu9d3gL1NVE0VkB9BOVUt847eqapKIZAEpqlrkN4/OwEfqHjCCiNwBRKjq30XkfSAXd8uIN1U1N8CrakwlViMwpn60ltcHosjvdRn7+ujG4O4bMwj4zu+OmsYcEpYIjKmfi/3+f+V7/SXuzqcAE4AvfK8/Aa6Himcqt6htpiISBhylqvOAO4AWQLVaiTGBZGcexuzTTEQW+w2/r6rll5AmiMgS3Fn9eN+43+KeFvZ73JPDrvSNvwmYLiJX4c78r8fdVbImHuB5X7IQYJq6x08ac8hYH4ExdfD1EaSp6o5gx2JMIFjTkDHGhDirERhjTIizGoExxoQ4SwTGGBPiLBEYY0yIs0RgjDEhzhKBMcaEuP8PXY2csGjkTYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,101)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96c89b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:23:24.416735Z",
     "iopub.status.busy": "2022-09-25T13:23:24.416024Z",
     "iopub.status.idle": "2022-09-25T13:23:24.422665Z",
     "shell.execute_reply": "2022-09-25T13:23:24.421751Z"
    },
    "papermill": {
     "duration": 0.261735,
     "end_time": "2022-09-25T13:23:24.425228",
     "exception": false,
     "start_time": "2022-09-25T13:23:24.163493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3278793692588806"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4fb5a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:23:24.941680Z",
     "iopub.status.busy": "2022-09-25T13:23:24.941324Z",
     "iopub.status.idle": "2022-09-25T13:23:24.947598Z",
     "shell.execute_reply": "2022-09-25T13:23:24.946617Z"
    },
    "papermill": {
     "duration": 0.274735,
     "end_time": "2022-09-25T13:23:24.949968",
     "exception": false,
     "start_time": "2022-09-25T13:23:24.675233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47298672795295715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721b3d8",
   "metadata": {
    "papermill": {
     "duration": 0.256672,
     "end_time": "2022-09-25T13:23:25.473356",
     "exception": false,
     "start_time": "2022-09-25T13:23:25.216684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Which optimizer should i use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142edb3e",
   "metadata": {
    "papermill": {
     "duration": 0.299286,
     "end_time": "2022-09-25T13:23:26.032449",
     "exception": false,
     "start_time": "2022-09-25T13:23:25.733163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7748d8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:23:26.544134Z",
     "iopub.status.busy": "2022-09-25T13:23:26.543704Z",
     "iopub.status.idle": "2022-09-25T13:23:26.550175Z",
     "shell.execute_reply": "2022-09-25T13:23:26.549179Z"
    },
    "papermill": {
     "duration": 0.265963,
     "end_time": "2022-09-25T13:23:26.552913",
     "exception": false,
     "start_time": "2022-09-25T13:23:26.286950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:tf.config.experimental.set_virtual_device_configuration(\n",
    "    \n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27830ba",
   "metadata": {
    "papermill": {
     "duration": 0.25618,
     "end_time": "2022-09-25T13:23:27.059109",
     "exception": false,
     "start_time": "2022-09-25T13:23:26.802929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A MUST BE : n_jobs=1 not -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2606b5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:23:27.633336Z",
     "iopub.status.busy": "2022-09-25T13:23:27.632966Z",
     "iopub.status.idle": "2022-09-25T13:31:15.558550Z",
     "shell.execute_reply": "2022-09-25T13:31:15.557363Z"
    },
    "papermill": {
     "duration": 468.182949,
     "end_time": "2022-09-25T13:31:15.562186",
     "exception": false,
     "start_time": "2022-09-25T13:23:27.379237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epochs', 'batch_size', 'verbose', 'build_fn'])\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "def getmodel(optimizer):\n",
    "    \n",
    "    model0 = ks.models.Sequential()\n",
    "    model0.add(ks.layers.Dense(4, input_shape=(64, 64, 1))) #units=4(we define how many outputs we want)\n",
    "    model0.add(ks.layers.Dense(8, input_shape=(64, 64, 1)))\n",
    "    model0.add(ks.layers.Dense(8, input_shape=(64, 64, 1)))\n",
    "    model0.add(ks.layers.Conv2D(64,(3, 3), activation='relu')) \n",
    "       \n",
    "    model0.add(ks.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "    model0.add(ks.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model0.add(ks.layers.MaxPooling2D(2, 2)) \n",
    "       \n",
    "    model0.add(ks.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model0.add(ks.layers.MaxPooling2D(2, 2))\n",
    "    model0.add(ks.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "    model0.add(ks.layers.MaxPooling2D(2, 2))\n",
    "    model0.add(ks.layers.Dropout(0.2))\n",
    "    model0.add(ks.layers.Flatten())\n",
    "\n",
    "    model0.add(ks.layers.Dense(1024, activation='relu'))\n",
    "    model0.add(ks.layers.Dense(10, activation='softmax'))\n",
    "    model0.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['acc'])\n",
    "    return model0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks = myCallback()\n",
    "batch_size=5\n",
    "epochs=20\n",
    "#create model\n",
    "model = KerasClassifier(build_fn=getmodel , epochs = epochs, batch_size=batch_size , verbose=0)\n",
    "print(model.get_params().keys())\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'Adam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid = param_grid , n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(train_x,train_y) #naateha ken train zaama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cfbd66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:31:16.128516Z",
     "iopub.status.busy": "2022-09-25T13:31:16.127797Z",
     "iopub.status.idle": "2022-09-25T13:31:16.134426Z",
     "shell.execute_reply": "2022-09-25T13:31:16.133354Z"
    },
    "papermill": {
     "duration": 0.263394,
     "end_time": "2022-09-25T13:31:16.138513",
     "exception": false,
     "start_time": "2022-09-25T13:31:15.875119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.746201 using {'optimizer': 'SGD'}\n",
      "0.746201 (0.009313) with: {'optimizer': 'SGD'}\n",
      "0.332289 (0.313831) with: {'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d37659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:31:16.682902Z",
     "iopub.status.busy": "2022-09-25T13:31:16.682526Z",
     "iopub.status.idle": "2022-09-25T13:31:16.689124Z",
     "shell.execute_reply": "2022-09-25T13:31:16.688192Z"
    },
    "papermill": {
     "duration": 0.302606,
     "end_time": "2022-09-25T13:31:16.692047",
     "exception": false,
     "start_time": "2022-09-25T13:31:16.389441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.746201 using {'optimizer': 'SGD'}\n",
      "0.746201 (0.009313) with: {'optimizer': 'SGD'}\n",
      "0.332289 (0.313831) with: {'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b9e69",
   "metadata": {
    "papermill": {
     "duration": 0.260611,
     "end_time": "2022-09-25T13:31:17.203853",
     "exception": false,
     "start_time": "2022-09-25T13:31:16.943242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e21b48bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:31:17.703123Z",
     "iopub.status.busy": "2022-09-25T13:31:17.702011Z",
     "iopub.status.idle": "2022-09-25T13:31:17.826247Z",
     "shell.execute_reply": "2022-09-25T13:31:17.825297Z"
    },
    "papermill": {
     "duration": 0.374791,
     "end_time": "2022-09-25T13:31:17.828628",
     "exception": false,
     "start_time": "2022-09-25T13:31:17.453837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model0_regularized = ks.models.Sequential()\n",
    "model0_regularized.add(ks.layers.Dense(4, input_shape=(64, 64, 1), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) #units=4(we define how many outputs we want)\n",
    "model0_regularized.add(ks.layers.Dense(8, input_shape=(64, 64, 1), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model0_regularized.add(ks.layers.Dense(8, input_shape=(64, 64, 1), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model0_regularized.add(ks.layers.Conv2D(64,(3, 3), activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
    "\n",
    "model0_regularized.add(ks.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "model0_regularized.add(ks.layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model0_regularized.add(ks.layers.MaxPooling2D((2, 2))) \n",
    "       \n",
    "model0_regularized.add(ks.layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model0_regularized.add(ks.layers.MaxPooling2D(2, 2))\n",
    "model0_regularized.add(ks.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model0_regularized.add(ks.layers.MaxPooling2D(2, 2))\n",
    "model0_regularized.add(ks.layers.Dropout(0.2))\n",
    "model0_regularized.add(ks.layers.Flatten())\n",
    "\n",
    "model0_regularized.add(ks.layers.Dense(1024, activation='relu', kernel_regularizer=l2(0.01) , bias_regularizer=l2(0.01)))\n",
    "model0_regularized.add(ks.layers.Dense(10, activation='softmax', kernel_regularizer=l2(0.01) ,  bias_regularizer=l2(0.01)))\n",
    "#i'm not sure about flatten maxpooling2d\n",
    "\n",
    "model0_regularized.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "                      # max pooling error for L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596669ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:31:18.331025Z",
     "iopub.status.busy": "2022-09-25T13:31:18.330655Z",
     "iopub.status.idle": "2022-09-25T13:31:18.340478Z",
     "shell.execute_reply": "2022-09-25T13:31:18.339667Z"
    },
    "papermill": {
     "duration": 0.263497,
     "end_time": "2022-09-25T13:31:18.342479",
     "exception": false,
     "start_time": "2022-09-25T13:31:18.078982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model0_regularized.compile(optimizer=ks.optimizers.Adam(),loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee83b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:31:18.885292Z",
     "iopub.status.busy": "2022-09-25T13:31:18.884845Z",
     "iopub.status.idle": "2022-09-25T13:37:10.653998Z",
     "shell.execute_reply": "2022-09-25T13:37:10.652953Z"
    },
    "papermill": {
     "duration": 352.022308,
     "end_time": "2022-09-25T13:37:10.656597",
     "exception": false,
     "start_time": "2022-09-25T13:31:18.634289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 4s 25ms/step - loss: 4.2331 - acc: 0.1158 - val_loss: 2.4167 - val_acc: 0.1053\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3832 - acc: 0.1151 - val_loss: 2.3589 - val_acc: 0.1053\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3438 - acc: 0.1151 - val_loss: 2.3318 - val_acc: 0.1053\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 2.3232 - acc: 0.1151 - val_loss: 2.3170 - val_acc: 0.1053\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3121 - acc: 0.1151 - val_loss: 2.3092 - val_acc: 0.1053\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3062 - acc: 0.1151 - val_loss: 2.3051 - val_acc: 0.1053\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3032 - acc: 0.1151 - val_loss: 2.3031 - val_acc: 0.1053\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3017 - acc: 0.1151 - val_loss: 2.3021 - val_acc: 0.1053\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3010 - acc: 0.1151 - val_loss: 2.3017 - val_acc: 0.1053\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3006 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3005 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3003 - acc: 0.1151 - val_loss: 2.3013 - val_acc: 0.1053\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3015 - val_acc: 0.1053\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3003 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3003 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3004 - acc: 0.1151 - val_loss: 2.3015 - val_acc: 0.1053\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 4s 29ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 4s 30ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 4s 31ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 4s 28ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 4s 29ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 4s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 4s 27ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 4s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 3s 24ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 3s 25ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 4s 29ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 3s 23ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 3s 26ms/step - loss: 2.3001 - acc: 0.1151 - val_loss: 2.3014 - val_acc: 0.1053\n"
     ]
    }
   ],
   "source": [
    "callbacks = myCallback()\n",
    "batch_size=5\n",
    "FAST_RUN = False\n",
    "epochs=20 if FAST_RUN else 100\n",
    "history = model0_regularized.fit_generator(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[\n",
    "        ks.callbacks.ReduceLROnPlateau()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4050f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:37:11.713698Z",
     "iopub.status.busy": "2022-09-25T13:37:11.713325Z",
     "iopub.status.idle": "2022-09-25T13:37:11.924559Z",
     "shell.execute_reply": "2022-09-25T13:37:11.923487Z"
    },
    "papermill": {
     "duration": 0.764739,
     "end_time": "2022-09-25T13:37:11.927064",
     "exception": false,
     "start_time": "2022-09-25T13:37:11.162325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,101)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3813371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:37:12.969770Z",
     "iopub.status.busy": "2022-09-25T13:37:12.969406Z",
     "iopub.status.idle": "2022-09-25T13:37:12.976105Z",
     "shell.execute_reply": "2022-09-25T13:37:12.975078Z"
    },
    "papermill": {
     "duration": 0.552424,
     "end_time": "2022-09-25T13:37:12.978068",
     "exception": false,
     "start_time": "2022-09-25T13:37:12.425644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3013460636138916"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6e2250c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:37:13.972097Z",
     "iopub.status.busy": "2022-09-25T13:37:13.971729Z",
     "iopub.status.idle": "2022-09-25T13:37:13.977923Z",
     "shell.execute_reply": "2022-09-25T13:37:13.977024Z"
    },
    "papermill": {
     "duration": 0.506993,
     "end_time": "2022-09-25T13:37:13.979974",
     "exception": false,
     "start_time": "2022-09-25T13:37:13.472981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3001186847686768"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf52a4",
   "metadata": {
    "papermill": {
     "duration": 0.498778,
     "end_time": "2022-09-25T13:37:15.025095",
     "exception": false,
     "start_time": "2022-09-25T13:37:14.526317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can wsee the validation loss has decreased :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "979b9415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T13:37:16.062674Z",
     "iopub.status.busy": "2022-09-25T13:37:16.062316Z",
     "iopub.status.idle": "2022-09-25T13:37:16.269770Z",
     "shell.execute_reply": "2022-09-25T13:37:16.268794Z"
    },
    "papermill": {
     "duration": 0.710564,
     "end_time": "2022-09-25T13:37:16.271851",
     "exception": false,
     "start_time": "2022-09-25T13:37:15.561287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = history.history['loss']\n",
    "epochs = range(1,101)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde9018",
   "metadata": {
    "papermill": {
     "duration": 0.570354,
     "end_time": "2022-09-25T13:37:17.341549",
     "exception": false,
     "start_time": "2022-09-25T13:37:16.771195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# L2 Regularization (for Machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d3e04",
   "metadata": {
    "papermill": {
     "duration": 0.685752,
     "end_time": "2022-09-25T13:37:18.523082",
     "exception": false,
     "start_time": "2022-09-25T13:37:17.837330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A regression model that uses **L1 regularization** technique is called **Lasso Regression** and model which uses **L2** is called **Ridge Regression.**  \n",
    "The key difference between these two is the penalty term.\n",
    "Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with **a large set of features.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b374954",
   "metadata": {
    "papermill": {
     "duration": 0.495752,
     "end_time": "2022-09-25T13:37:19.641487",
     "exception": false,
     "start_time": "2022-09-25T13:37:19.145735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "If you have studied the concept of regularization in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.\n",
    "\n",
    "Assume that our regularization coefficient is so high that some of the weight matrices are nearly equal to zero. This will result in a much simpler linear network and slight underfitting of the training data.\n",
    "\n",
    "Such a large value of the regularization coefficient is not that useful. We need to optimize the value of regularization coefficient in order to obtain a well-fitted model as shown in the image below.  \n",
    "\n",
    "**Variables standardization** is the initial procedure in ridge regression. Both the independent and dependent variables require standardization through subtraction of their averages and a division of the result with the standard deviations. It is common practice to annotate in a formula whether the variables therein are standardized or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42e411",
   "metadata": {
    "papermill": {
     "duration": 0.785949,
     "end_time": "2022-09-25T13:37:21.113746",
     "exception": false,
     "start_time": "2022-09-25T13:37:20.327797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "regularizing lamba part is in progress ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843c4d1",
   "metadata": {
    "papermill": {
     "duration": 0.50129,
     "end_time": "2022-09-25T13:37:22.210593",
     "exception": false,
     "start_time": "2022-09-25T13:37:21.709303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To find optimum alpha for Ridge Regularization we can be applying GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1217.081916,
   "end_time": "2022-09-25T13:37:25.780793",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-25T13:17:08.698877",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
